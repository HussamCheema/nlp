{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"At eight o'clock on Thursday morning\n",
    "... Arthur didn't feel very good. Hola Madrida!!\n",
    "My name is Hussam and I'm doing Natural Language Processing work right now.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"At eight o'clock on Thursday morning\\nArthur didn't feel very good.\", 'Hola Madrida!!', \"My name is Hussam and I'm doing Natural Language Processing work right now.\"]\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.', 'Hola', 'Madrida', '!', '!', 'My', 'name', 'is', 'Hussam', 'and', 'I', \"'m\", 'doing', 'Natural', 'Language', 'Processing', 'work', 'right', 'now', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i', 'he', 'mightn', 'each', 'nor', 'again', 'their', 'yourselves', 'while', 'off', 'same', 'than', 'very', 'over', 'who', 'on', 'and', 'from', 'hasn', 'wasn', 'ma', 't', 'couldn', 'with', 'until', \"hasn't\", \"you've\", 'she', 'of', 'an', 'your', 'her', 'been', \"shan't\", \"it's\", \"mightn't\", 'it', 'them', 'having', 'y', 'by', 'when', 'we', 'can', 'm', 'mustn', 'o', \"should've\", 'more', 'if', 'needn', 'for', 'yours', 'once', 'should', 'all', 'those', 'did', \"couldn't\", \"doesn't\", 'didn', 'd', \"she's\", \"shouldn't\", 'then', 'is', 'has', \"you'd\", 'such', 'to', 'both', 'isn', 'a', 'because', \"wasn't\", \"isn't\", 'between', 'what', 'whom', 'some', 're', 'this', 'haven', 'most', 'that', \"won't\", \"needn't\", 'but', 'shan', 'ours', 'being', 'herself', 'where', 'ain', 'or', 'don', 'himself', 'they', 'other', 'below', 'not', 'have', 'doing', 'does', 'further', 'about', 'there', 'won', 'ourselves', \"you're\", 'in', \"wouldn't\", 'only', 'shouldn', 'was', 'into', 'theirs', 'doesn', 'here', 'themselves', 'out', 'so', \"haven't\", 'his', 'why', 'now', 'am', 'myself', 'll', 've', 'itself', 'the', 'had', 'these', 'above', 'any', 'few', \"didn't\", 'are', 'just', 'will', 'my', 'which', 'wouldn', 'be', 'own', 'before', 'at', 'weren', 'down', \"you'll\", 'during', 'no', 'were', \"weren't\", 'through', 'under', 'too', 'him', 'you', \"mustn't\", \"that'll\", \"don't\", 'as', \"aren't\", 'against', 'our', 'hadn', 'me', 'yourself', 'its', 'up', 's', 'how', \"hadn't\", 'hers', 'aren', 'after', 'do'}\n"
     ]
    }
   ],
   "source": [
    "# Stop Words (which repeat alot)\n",
    "print(set(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believ'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E.g. \n",
    "stemmer.stem(\"Believe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at eight o'clock thursday morn arthur n't feel good .\n",
      "hola madrida ! !\n",
      "my name hussam i 'm natur languag process work right .\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    stems = [stemmer.stem(i) for i in words if i not in set(stopwords.words(\"english\"))]\n",
    "    new_sentence = \" \".join(stems)\n",
    "    print(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Believe'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E.g.\n",
    "lemmatizer.lemmatize(\"Believe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At eight o'clock Thursday morning Arthur n't feel good .\n",
      "Hola Madrida ! !\n",
      "My name Hussam I 'm Natural Language Processing work right .\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    stems = [lemmatizer.lemmatize(i) for i in words if i not in set(stopwords.words(\"english\"))]\n",
    "    new_sentence = \" \".join(stems)\n",
    "    print(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words (To create Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph =  \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization can also be used here \n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 1 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Creating the Bag of Words model\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf (To create Embeddings)\n",
    "#### (Better than BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "# Lemmatization can also be used here \n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.25883507 0.30512561 0.        ]\n",
      " [0.         0.28867513 0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "X = tf_idf_vectorizer.fit_transform(corpus).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (To create Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blog Post: https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['three', 'visions', 'india', '.'],\n",
       " ['years',\n",
       "  'history',\n",
       "  ',',\n",
       "  'people',\n",
       "  'world',\n",
       "  'come',\n",
       "  'invaded',\n",
       "  'us',\n",
       "  ',',\n",
       "  'captured',\n",
       "  'lands',\n",
       "  ',',\n",
       "  'conquered',\n",
       "  'minds',\n",
       "  '.'],\n",
       " ['alexander',\n",
       "  'onwards',\n",
       "  ',',\n",
       "  'greeks',\n",
       "  ',',\n",
       "  'turks',\n",
       "  ',',\n",
       "  'moguls',\n",
       "  ',',\n",
       "  'portuguese',\n",
       "  ',',\n",
       "  'british',\n",
       "  ',',\n",
       "  'french',\n",
       "  ',',\n",
       "  'dutch',\n",
       "  ',',\n",
       "  'came',\n",
       "  'looted',\n",
       "  'us',\n",
       "  ',',\n",
       "  'took',\n",
       "  '.'],\n",
       " ['yet', 'done', 'nation', '.'],\n",
       " ['conquered', 'anyone', '.'],\n",
       " ['grabbed',\n",
       "  'land',\n",
       "  ',',\n",
       "  'culture',\n",
       "  ',',\n",
       "  'history',\n",
       "  'tried',\n",
       "  'enforce',\n",
       "  'way',\n",
       "  'life',\n",
       "  '.'],\n",
       " ['?'],\n",
       " ['respect', 'freedom', 'others.that', 'first', 'vision', 'freedom', '.'],\n",
       " ['believe',\n",
       "  'india',\n",
       "  'got',\n",
       "  'first',\n",
       "  'vision',\n",
       "  ',',\n",
       "  'started',\n",
       "  'war',\n",
       "  'independence',\n",
       "  '.'],\n",
       " ['freedom', 'must', 'protect', 'nurture', 'build', '.'],\n",
       " ['free', ',', 'one', 'respect', 'us', '.'],\n",
       " ['second', 'vision', 'india', '’', 'development', '.'],\n",
       " ['fifty', 'years', 'developing', 'nation', '.'],\n",
       " ['time', 'see', 'developed', 'nation', '.'],\n",
       " ['among', 'top', 'nations', 'world', 'terms', 'gdp', '.'],\n",
       " ['percent', 'growth', 'rate', 'areas', '.'],\n",
       " ['poverty', 'levels', 'falling', '.'],\n",
       " ['achievements', 'globally', 'recognised', 'today', '.'],\n",
       " ['yet',\n",
       "  'lack',\n",
       "  'self-confidence',\n",
       "  'see',\n",
       "  'developed',\n",
       "  'nation',\n",
       "  ',',\n",
       "  'self-reliant',\n",
       "  'self-assured',\n",
       "  '.'],\n",
       " ['’', 'incorrect', '?'],\n",
       " ['third', 'vision', '.'],\n",
       " ['india', 'must', 'stand', 'world', '.'],\n",
       " ['believe',\n",
       "  'unless',\n",
       "  'india',\n",
       "  'stands',\n",
       "  'world',\n",
       "  ',',\n",
       "  'one',\n",
       "  'respect',\n",
       "  'us',\n",
       "  '.'],\n",
       " ['strength', 'respects', 'strength', '.'],\n",
       " ['must', 'strong', 'military', 'power', 'also', 'economic', 'power', '.'],\n",
       " ['must', 'go', 'hand-in-hand', '.'],\n",
       " ['good', 'fortune', 'worked', 'three', 'great', 'minds', '.'],\n",
       " ['dr.', 'vikram', 'sarabhai', 'dept', '.'],\n",
       " ['space',\n",
       "  ',',\n",
       "  'professor',\n",
       "  'satish',\n",
       "  'dhawan',\n",
       "  ',',\n",
       "  'succeeded',\n",
       "  'dr.',\n",
       "  'brahm',\n",
       "  'prakash',\n",
       "  ',',\n",
       "  'father',\n",
       "  'nuclear',\n",
       "  'material',\n",
       "  '.'],\n",
       " ['lucky',\n",
       "  'worked',\n",
       "  'three',\n",
       "  'closely',\n",
       "  'consider',\n",
       "  'great',\n",
       "  'opportunity',\n",
       "  'life',\n",
       "  '.'],\n",
       " ['see', 'four', 'milestones', 'career']]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', ',', 'india', 'vision', 'must', 'nation', 'world', 'us', 'three', 'freedom', 'respect', 'see', 'first', 'power', 'yet', '’', 'strength', 'worked', '?', 'life', 'believe', 'dr.', 'great', 'minds', 'one', 'years', 'history', 'developed', 'conquered', 'protect', 'others.that', 'war', 'nurture', 'independence', 'build', 'free', 'tried', 'got', 'started', 'way', 'enforce', 'took', 'culture', 'turks', 'visions', 'people', 'come', 'invaded', 'captured', 'lands', 'alexander', 'onwards', 'greeks', 'moguls', 'land', 'portuguese', 'british', 'french', 'dutch', 'came', 'looted', 'done', 'anyone', 'grabbed', 'second', 'career', 'development', 'respects', 'military', 'also', 'economic', 'go', 'hand-in-hand', 'good', 'fortune', 'vikram', 'sarabhai', 'dept', 'space', 'professor', 'satish', 'dhawan', 'succeeded', 'brahm', 'prakash', 'father', 'nuclear', 'material', 'lucky', 'closely', 'consider', 'opportunity', 'four', 'strong', 'stands', 'fifty', 'unless', 'developing', 'milestones', 'among', 'top', 'nations', 'terms', 'gdp', 'percent', 'growth', 'rate', 'areas', 'poverty', 'levels', 'falling', 'achievements', 'globally', 'recognised', 'today', 'lack', 'self-confidence', 'self-reliant', 'self-assured', 'incorrect', 'third', 'stand', 'time']\n"
     ]
    }
   ],
   "source": [
    "words = model.wv.index_to_key\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00219905 -0.00970885  0.00929075  0.00203636 -0.00116388 -0.00551955\n",
      " -0.0085126  -0.00989383  0.00894091 -0.00250522  0.00459427 -0.00452481\n",
      "  0.00995189  0.00366171  0.00103129 -0.00403834  0.00122027 -0.00265451\n",
      "  0.00735284  0.00447542  0.00099955  0.0034782   0.00372712 -0.00680036\n",
      "  0.00893242  0.00173499 -0.00579935  0.00866838 -0.00129286  0.00818304\n",
      " -0.0014927   0.00698649  0.00273452 -0.00436226 -0.00374683  0.00919046\n",
      "  0.00159645 -0.00599784  0.00034776 -0.00195135  0.00159242 -0.00771525\n",
      "  0.00738298  0.00131083  0.00787099  0.00445568 -0.00439675  0.00376054\n",
      " -0.0006357  -0.00984484  0.00825004  0.00964326  0.00965426 -0.00379659\n",
      " -0.00844202  0.00483581 -0.00765107  0.00853567  0.00275977  0.00560496\n",
      "  0.00611362  0.00046455 -0.00209463  0.000778    0.00983559 -0.00711718\n",
      " -0.00155744 -0.00235984  0.00487084  0.00645515 -0.0041403   0.00361816\n",
      " -0.00447258  0.00326611  0.0081738   0.00361967 -0.0045711  -0.00301938\n",
      "  0.00787179  0.00959686  0.00580789 -0.00326881 -0.00183876 -0.00624998\n",
      " -0.00429521  0.00336554 -0.00648911 -0.00661903  0.00811393  0.00950739\n",
      "  0.00814451  0.00150699 -0.00880125 -0.00759764  0.0015789  -0.00952675\n",
      " -0.00741688  0.00203283 -0.00292885 -0.00916266]\n"
     ]
    }
   ],
   "source": [
    "# Finding Word Vector/Embedding\n",
    "vector = model.wv['war']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('turks', 0.21573834121227264), ('yet', 0.1890701949596405), ('lands', 0.18791650235652924), ('rate', 0.17437395453453064), ('culture', 0.17353151738643646), ('developed', 0.1681673228740692), ('?', 0.16697141528129578), ('worked', 0.15106210112571716), ('four', 0.12725570797920227), ('one', 0.12150037288665771)]\n"
     ]
    }
   ],
   "source": [
    "# Most similar words\n",
    "similar = model.wv.most_similar('nuclear')\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings using Keras Embedding Layer\n",
    "#### (Have sementic information - also capture relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sentences\n",
    "sentences=[  'the glass of milk',\n",
    "     'the glass of juice',\n",
    "     'the cup of tea',\n",
    "     'I am a good boy',\n",
    "     'I am a good developer',\n",
    "     'understand the meaning of words',\n",
    "     'your videos are good',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6591, 4131, 1467, 4053], [6591, 4131, 1467, 6908], [6591, 5105, 1467, 7840], [1793, 8152, 3383, 433, 8331], [1793, 8152, 3383, 433, 928], [9780, 6591, 2776, 1467, 4863], [1864, 5154, 2081, 433]]\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encoding (Have no sementic information)\n",
    "onehot_repr=[one_hot(sentence,voc_size)for sentence in sentences] \n",
    "print(onehot_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0 6591 4131 1467 4053]\n",
      " [   0    0    0    0 6591 4131 1467 6908]\n",
      " [   0    0    0    0 6591 5105 1467 7840]\n",
      " [   0    0    0 1793 8152 3383  433 8331]\n",
      " [   0    0    0 1793 8152 3383  433  928]\n",
      " [   0    0    0 9780 6591 2776 1467 4863]\n",
      " [   0    0    0    0 1864 5154 2081  433]]\n"
     ]
    }
   ],
   "source": [
    "sent_length=8\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,dim,input_length=sent_length))\n",
    "model.compile('adam','mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 8, 10)             100000    \n",
      "=================================================================\n",
      "Total params: 100,000\n",
      "Trainable params: 100,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0 6591 4131 1467 4053]\n",
      "[[-0.00224646  0.03331555 -0.02420985 -0.02792797  0.0268615  -0.03484186\n",
      "   0.0230451  -0.02155731 -0.00500866 -0.03395703]\n",
      " [-0.00224646  0.03331555 -0.02420985 -0.02792797  0.0268615  -0.03484186\n",
      "   0.0230451  -0.02155731 -0.00500866 -0.03395703]\n",
      " [-0.00224646  0.03331555 -0.02420985 -0.02792797  0.0268615  -0.03484186\n",
      "   0.0230451  -0.02155731 -0.00500866 -0.03395703]\n",
      " [-0.00224646  0.03331555 -0.02420985 -0.02792797  0.0268615  -0.03484186\n",
      "   0.0230451  -0.02155731 -0.00500866 -0.03395703]\n",
      " [-0.02919956 -0.00889396 -0.03809033 -0.02981687 -0.02044038  0.0344638\n",
      "  -0.01321349  0.00214417  0.01543659  0.02634079]\n",
      " [ 0.01386091  0.02302313  0.04099892  0.00788177 -0.02506766  0.00903445\n",
      "   0.01393671 -0.0310407   0.01009542  0.02513399]\n",
      " [-0.00043292 -0.04105276  0.03874649  0.01637181 -0.04665422  0.00538928\n",
      "  -0.01397682  0.01599056 -0.01491641 -0.04338221]\n",
      " [ 0.04760106  0.00921056 -0.03760391 -0.0053262  -0.02699087 -0.04466764\n",
      "  -0.00174364  0.03752716 -0.04425213  0.00755063]]\n"
     ]
    }
   ],
   "source": [
    "print(embedded_docs[0])\n",
    "print(model.predict(embedded_docs)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRP'), ('is', 'VBZ'), ('Hussam', 'NNP'), ('.', '.'), ('He', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('guy', 'NN'), ('and', 'CC'), ('friend', 'NN'), ('of', 'IN'), ('Usman', 'NNP'), ('.', '.'), ('Hussam', 'NNP'), ('works', 'VBZ'), ('in', 'IN'), ('GOOGLE', 'NNP'), ('as', 'IN'), ('Software Engineer', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = nltk.pos_tag([\"He\",\"is\",\"Hussam\",\".\",\"He\",\"is\",\"a\",\"great\",\"guy\",\"and\",\n",
    "                         \"friend\",\"of\", \"Usman\",\".\",\"Hussam\",\"works\",\"in\",\"GOOGLE\",\"as\",\"Software Engineer\"])\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  He/PRP\n",
      "  is/VBZ\n",
      "  (PERSON Hussam/NNP)\n",
      "  ./.\n",
      "  He/PRP\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  great/JJ\n",
      "  guy/NN\n",
      "  and/CC\n",
      "  friend/NN\n",
      "  of/IN\n",
      "  (GPE Usman/NNP)\n",
      "  ./.\n",
      "  (PERSON Hussam/NNP)\n",
      "  works/VBZ\n",
      "  in/IN\n",
      "  (ORGANIZATION GOOGLE/NNP)\n",
      "  as/IN\n",
      "  Software Engineer/NNP)\n"
     ]
    }
   ],
   "source": [
    "named_entities = nltk.ne_chunk(pos_tags)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
